# -*- coding: utf-8 -*-
"""modeloAE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A8ryfyz4NUV90QQMjRy8iXr5Zi3ikdjz

VGG

![vgg.JPG](attachment:60320344-3545-41ba-8f67-677e05e93da3.JPG)

![AEconvulucionalArquitecturaJPG.JPG](attachment:f4da17ae-4059-4682-b406-3e830bc37c06.JPG)
"""

########################################################################
# ENCODERS PARA TENSORES
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

########################################################################
# 2. Define a Convolution Neural Network (Encoder) 28*28
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch
import numpy as np

class Encoder28(nn.Module):
    
    def __init__(self, codesize):
        super().__init__()
        
        # Encoder specification
        self.pool = nn.MaxPool2d(2, 2)
        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, codesize, 3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv6 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv7 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        #### self.Dropout02 = nn.Dropout(0.2)

    def forward(self, x):
        code = self.encode(x)
        return code
    
    def encode(self, x):
        #print("x", x.shape)
        x = F.relu(self.norm1(self.conv1(x)))
        #print("conv 1", x.shape)
        x = F.relu(x + self.norm1(self.conv4(x)))
        #print("conv 4", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm2(self.conv2(x)))
        #print("conv 2", x.shape)
        x = F.relu(x + self.norm2(self.conv5(x)))
        #print("conv 5", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm3(self.conv3(x)))
        #print("conv 3", x.shape)
        x = F.relu(x + self.norm3(self.conv6(x)))
        #print("conv 6", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.sigmoid(x + self.norm3(self.conv7(x)))
        #print("conv 7", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.squeeze(x)
        #print("squeeze", x.shape)
        return x

########################################################################
# 2. Define a Convolution Neural Network (Encoder) 32*32
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch

class Encoder32(nn.Module):
    
    def __init__(self, codesize, nChanelsIn):
        super().__init__()
        
        # Encoder specification
        self.pool = nn.MaxPool2d(2, 2)
        self.conv1 = nn.Conv2d(nChanelsIn, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, codesize, 3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv6 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv7 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        
        #-----------------------------My encoder specification
        self.minorm16 = nn.BatchNorm2d(16, momentum=1, affine=True)
        self.miconv16 = nn.Conv2d(32, 16, 3, padding=1)
        self.miconv1616 = nn.Conv2d(16, 16, 3, padding=1)
        self.miconvd = nn.Conv2d(16, codesize, 3, padding=1) # El 16 creo que toca hacerlo variable para que se adapte a 32 (si es 28*28) o 16 (si es 32*32)
        
    def forward(self, x):
        code = self.encode(x)
        return code
    
    def encode(self, x):
        #print("x", x.shape) # [64, 1, 32, 32]
        x = F.relu(self.norm1(self.conv1(x))) # 1,64, 3,1
        #print("conv 1", x.shape) # [64, 64, 32, 32]
        x = F.relu(x + self.norm1(self.conv4(x))) # 64,64, 3,1
        #print("conv 4", x.shape) # [64, 64, 32, 32]
        x = self.pool(x) #2,2
        #print("pool", x.shape) #[64, 64, 16, 16]
        x = F.relu(self.norm2(self.conv2(x))) # 64,32, 3,1
        #print("conv 2", x.shape) # [64, 32, 16, 16]
        x = F.relu(x + self.norm2(self.conv5(x))) # 32,32, 3,1
        #print("conv 5", x.shape) # [64, 32, 16, 16]
        x = self.pool(x) 
        #print("pool", x.shape) # [64, 32, 8, 8]

        #----32*32  creo que aqui toca poner un if dependiendo si la imagen es 32*32 pone este bloque
        x = F.relu(self.minorm16(self.miconv16(x))) # 32 canales, 16 kernels, 3 strid, 1 padding
        #print("conv16",x.shape) #conv1616 torch.Size([64, 16, 8, 8])
        x = F.relu(x + self.minorm16(self.miconv1616(x))) # 16 canales, 16 kernels, 3,1
        #print("conv1616",x.shape) #conv1616 torch.Size([64, 16, 8, 8])
        x = self.pool(x) 
        #print("mi pool 3",x.shape) #mi pool 3 torch.Size([64, 16, 4, 4])
        x = F.relu(self.norm3(self.miconvd(x))) # 16,d, 3,1
        #print("miconvd", x.shape) # [64, 2, 4, 4]
        #----
        
        #x = F.relu(self.norm3(self.conv3(x))) # remplac√© la conv 3 (32,codsize) por miconvd (16,codsize) .... (creo que arriba micovd debe ser variable)
        #print("conv3", x.shape)
        
        x = F.relu(x + self.norm3(self.conv6(x))) # d,d, 3,1
        #print("conv 6", x.shape) # [64, 2, 4, 4]
        x = self.pool(x)
        #print("pool", x.shape) # [64, 2, 2, 2]
        x = torch.sigmoid(x + self.norm3(self.conv7(x))) # d,d, 3,1
        #print("conv 7", x.shape) # [64, 2, 2, 2]
        x = self.pool(x)
        #print("pool", x.shape) # [64, 2, 1, 1]
        x = torch.squeeze(x)
        #print("squeeze", x.shape) # [64, 2]

        return x

########################################################################
# 2. Define a Convolution Neural Network 28*20
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch

class Encoder2820(nn.Module):
    
    def __init__(self, codesize):
        super().__init__()
        
        # Encoder specification
        self.pool = nn.MaxPool2d(2, 2)
        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, codesize, 3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv6 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv7 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)

    def forward(self, x):
        code = self.encode(x)
        return code
    
    def encode(self, x):
        #print("Encode...")
        #print("x", x.shape)
        x = F.relu(self.norm1(self.conv1(x)))
        #print("conv 1", x.shape)
        x = F.relu(x + self.norm1(self.conv4(x)))
        #print("conv 4", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm2(self.conv2(x)))
        #print("conv 2", x.shape)
        x = F.relu(x + self.norm2(self.conv5(x)))
        #print("conv 5", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm3(self.conv3(x)))
        #print("conv 3", x.shape)
        x = F.relu(x + self.norm3(self.conv6(x)))
        #print("conv 6", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.sigmoid(x + self.norm3(self.conv7(x)))
        #print("conv 7", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.squeeze(x)
        #print("squeeze", x.shape)
        return x

########################################################################
# DECODERS PARA TENSORES
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

########################################################################
# 2. Define a Convolution Neural Network 28*28 (DECODER)
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch
import numpy as np

class Decoder28(nn.Module):
    
    def __init__(self, codesize):
        super().__init__()
        
        # Decoder specification
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        
        self.dconv1 = nn.ConvTranspose2d(codesize, 32, 3, stride=2)
        self.dconv2 = nn.ConvTranspose2d(32, 64, 2, stride=2)
        self.dconv3 = nn.ConvTranspose2d(64, 1, 2, stride=2)
        self.dconv4 = nn.ConvTranspose2d(codesize, codesize, 3, stride=2)
        self.conv8 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv9 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv10 = nn.Conv2d(64, 64, 3, padding=1)
        
        #### self.Dropout02 = nn.Dropout(0.2)

    def forward(self, x):
        out = self.decode(x)
        return out
    
    def decode(self, x):
        x = torch.unsqueeze(x, 2)
        x = torch.unsqueeze(x, 3)
        x = F.relu(self.norm3(self.dconv4(x)))
        x = F.relu(self.norm3(self.conv8(x)))
        x = F.relu(self.norm2(self.dconv1(x)))
        x = F.relu(self.norm2(self.conv9(x)))
        x = F.relu(self.norm1(self.dconv2(x)))
        x = F.relu(self.norm1(self.conv10(x)))
        x = self.norm4(self.dconv3(x))
        x = torch.tanh(x)
        return x

########################################################################
# 2. Define a Convolution Neural Network 32*32 (DECODER)
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch

class Decoder32(nn.Module):
    
    def __init__(self, codesize, nChanelsIn):
        super().__init__()
        
        # Decoder specification
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        
        self.dconv1 = nn.ConvTranspose2d(codesize, 32, 3, stride=2)
        self.dconv2 = nn.ConvTranspose2d(32, 64, 2, stride=2)
        self.dconv3 = nn.ConvTranspose2d(64, 1, 2, stride=2)
        self.dconv4 = nn.ConvTranspose2d(codesize, codesize, 2, stride=2)
        self.conv8 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv9 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv10 = nn.Conv2d(64, 64, 3, padding=1)
        
        #-----------------------------My decoder specification
        self.minorm16 = nn.BatchNorm2d(16, momentum=1, affine=True)
        self.midconv16 = nn.ConvTranspose2d(codesize, 16, 2, stride=2) # in_channels, out_channels, kernel_size, stride, padding
        self.midconv32 = nn.ConvTranspose2d(16, 32, 2, stride=2)
        self.miconv1616 = nn.Conv2d(16, 16, 3, padding=1)
        
    def forward(self, x):
        out = self.decode(x)
        return out
    
    def decode(self, x):
        #print("Decode...")
        #print("x d", x.shape) # [64, 2]
        x = torch.unsqueeze(x, 2)
        #print("unsqueeze", x.shape) # [64, 2, 1]
        x = torch.unsqueeze(x, 3)
        #print("unsqueeze", x.shape) # [64, 2, 1, 1]
        
        x = F.relu(self.norm3(self.dconv4(x))) # d,d, 2,2
        #print("d conv4", x.shape) # [64, 2, 3, 3]
        x = F.relu(self.norm3(self.conv8(x))) # d,d, 3,1
        #print("conv8", x.shape) # [64, 2, 3, 3]

        #----32*32
        x = F.relu(self.minorm16(self.midconv16(x))) # d,16, 2, 2
        #print("midcov16", x.shape)
        x = F.relu(self.minorm16(self.miconv1616(x))) # 16,16, 3,1
        #print("miconv1616", x.shape)
        x = F.relu(self.norm2(self.midconv32(x))) # 16,32, 2,2
        #print("midconv32", x.shape)
        #----
        
        #x = F.relu(self.norm2(self.dconv1(x))) # d,32, 3,2 remplace la dconv1 por  midconv1
        #print("d conv1", x.shape) # [64, 32, 7, 7]
        
        x = F.relu(self.norm2(self.conv9(x))) # 32,32, 3,1
        #print("conv9", x.shape) # [64, 32, 7, 7]
        x = F.relu(self.norm1(self.dconv2(x))) # 32,64, 2,2
        #print("d conv2", x.shape) # [64, 64, 14, 14]
        x = F.relu(self.norm1(self.conv10(x))) # 64,64, 3,1
        #print("conv10", x.shape) # [64, 64, 14, 14]
        x = self.norm4(self.dconv3(x))  # 64,1, 2,2
        #print("d conv3", x.shape) # [64, 1, 28, 28]
        
        x = torch.tanh(x) 
        #print("tanh", x.shape) # [64, 1, 32, 32]
        
        return x

########################################################################
# 2. Define a Convolution Neural Network 28*20 (DECODER)
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch

class Decoder2820(nn.Module):
    
    def __init__(self, codesize):
        super().__init__()
        
        # Decoder specification
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        
        self.dconv1 = nn.ConvTranspose2d(codesize, 32, 3, stride=2)  
        self.dconv2 = nn.ConvTranspose2d(32, 64, 2, stride=2)
        self.dconv3 = nn.ConvTranspose2d(64, 1, 2, stride=2)
        self.dconv4 = nn.ConvTranspose2d(codesize, codesize, kernel_size=(3, 2), stride=2)
        self.conv8 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv9 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv10 = nn.Conv2d(64, 64, 3, padding=1)

    def forward(self, x):
        out = self.decode(x)
        return out
    
    def decode(self, x):
        #print("Decode...")
        #print("xin", x.shape)
        x = torch.unsqueeze(x, 2)
        #print("unsqueeze", x.shape)
        x = torch.unsqueeze(x, 3)
        #print("unsqueeze", x.shape)
        x = F.relu(self.norm3(self.dconv4(x)))
        #print("dconv4", x.shape)
        x = F.relu(self.norm3(self.conv8(x)))
        #print("conv8", x.shape)
        x = F.relu(self.norm2(self.dconv1(x)))
        #print("dconv1", x.shape)
        x = F.relu(self.norm2(self.conv9(x)))
        #print("conv9", x.shape)
        x = F.relu(self.norm1(self.dconv2(x)))
        #print("dconv2", x.shape)
        x = F.relu(self.norm1(self.conv10(x)))
        #print("conv10", x.shape)
        x = self.norm4(self.dconv3(x))
        #print("dconv3", x.shape)
        x = torch.tanh(x)
        #print("tanh", x.shape)
        return x

########################################################################
# AUTOENCODERS PARA TENSORES
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

class AE(nn.Module):
    def __init__(self, enc, dec):
        super(AE, self).__init__()
        self.enc = enc
        self.dec = dec
        #self.classifier = nn.Linear(4, 2)
    
    def forward(self, x):
        code = self.enc(x)
        out = self.dec(code)
        return out, code

########################################################################
# 2. Define a Convolution Neural Network 28*28
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch
import numpy as np

class AEsequential28(nn.Module):
    
    def __init__(self, codesize):
        super().__init__()

        self.encode = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.BatchNorm2d(64, momentum=1, affine=True),
            nn.ReLU(True),
        
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64, momentum=1, affine=True),
            nn.ReLU(True),
        
            nn.MaxPool2d(2, 2),

            nn.Conv2d(64, 32, 3, padding=1),
            nn.BatchNorm2d(32, momentum=1, affine=True),
            nn.ReLU(True),
            
            nn.Conv2d(32, 32, 3, padding=1),
            nn.BatchNorm2d(32, momentum=1, affine=True),
            nn.ReLU(True),
        
            nn.MaxPool2d(2, 2),
        
            nn.Conv2d(32, codesize, 3, padding=1),
            nn.BatchNorm2d(codesize, momentum=1, affine=True),
            nn.ReLU(True),
        
            nn.Conv2d(codesize, codesize, 3, padding=1),
            nn.BatchNorm2d(codesize, momentum=1, affine=True),
            nn.ReLU(True),
        
            nn.MaxPool2d(2, 2),
        
            nn.Conv2d(codesize, codesize, 3, padding=1),
            nn.BatchNorm2d(codesize, momentum=1, affine=True),
        
            nn.Sigmoid(),
        
            nn.MaxPool2d(2, 2))
    
        self.decode = nn.Sequential(
            #x = torch.unsqueeze(x, 2)
            #x = torch.unsqueeze(x, 3)
        
            nn.ConvTranspose2d(codesize, codesize, 3, stride=2),
            nn.BatchNorm2d(codesize, momentum=1, affine=True),
            nn.ReLU(True),
        
            nn.Conv2d(codesize, codesize, 3, padding=1),
            nn.BatchNorm2d(codesize, momentum=1, affine=True),
            nn.ReLU(True),
        
            nn.ConvTranspose2d(codesize, 32, 3, stride=2),
            nn.BatchNorm2d(32, momentum=1, affine=True),
            nn.ReLU(True),
        
            nn.Conv2d(32, 32, 3, padding=1),
            nn.BatchNorm2d(32, momentum=1, affine=True),
            nn.ReLU(True),
        
            nn.ConvTranspose2d(32, 64, 2, stride=2),
            nn.BatchNorm2d(64, momentum=1, affine=True),
            nn.ReLU(True),
        
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64, momentum=1, affine=True),
            nn.ReLU(True),
        
            nn.ConvTranspose2d(64, 1, 2, stride=2),
            nn.BatchNorm2d(1, momentum=1, affine=True),
        
            nn.Tanh())
    
    def forward(self, x):
        code = torch.squeeze(self.encode(x))
        out = torch.unsqueeze(code, 2)
        out = torch.unsqueeze(out, 3)
        out = self.decode(out)
        return out, code

########################################################################
# 2. Define a Convolution Neural Network 28*28
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch
import numpy as np

class AutoEncoder28(nn.Module):
    
    def __init__(self, codesize):
        super().__init__()
        
        # Encoder specification
        self.pool = nn.MaxPool2d(2, 2)
        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, codesize, 3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv6 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv7 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        
        # Decoder specification
        self.dconv1 = nn.ConvTranspose2d(codesize, 32, 3, stride=2)
        self.dconv2 = nn.ConvTranspose2d(32, 64, 2, stride=2)
        self.dconv3 = nn.ConvTranspose2d(64, 1, 2, stride=2)
        self.dconv4 = nn.ConvTranspose2d(codesize, codesize, 3, stride=2)
        self.conv8 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv9 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv10 = nn.Conv2d(64, 64, 3, padding=1)
        
        #### self.Dropout02 = nn.Dropout(0.2)

    def forward(self, x):
        code = self.encode(x)
        out = self.decode(code)
        return out, code
    
    def encode(self, x):
        #print("x", x.shape)
        x = F.relu(self.norm1(self.conv1(x)))
        #print("conv 1", x.shape)
        x = F.relu(self.norm1(self.conv4(x)))
        #print("conv 4", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm2(self.conv2(x)))
        #print("conv 2", x.shape)
        x = F.relu(self.norm2(self.conv5(x)))
        #print("conv 5", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm3(self.conv3(x)))
        #print("conv 3", x.shape)
        x = F.relu(self.norm3(self.conv6(x)))
        #print("conv 6", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.sigmoid(self.norm3(self.conv7(x)))
        
        #print("conv 7", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.squeeze(x)
        #print("squeeze", x.shape)
        return x
    
    def decode(self, x):
        x = torch.unsqueeze(x, 2)
        x = torch.unsqueeze(x, 3)
        x = F.relu(self.norm3(self.dconv4(x)))
        x = F.relu(self.norm3(self.conv8(x)))
        x = F.relu(self.norm2(self.dconv1(x)))
        x = F.relu(self.norm2(self.conv9(x)))
        x = F.relu(self.norm1(self.dconv2(x)))
        x = F.relu(self.norm1(self.conv10(x)))
        x = self.norm4(self.dconv3(x))
        x = torch.tanh(x)
        return x
    
    def decodeRd(self, Vtrunc, xin):
        #x=kernel
        #X‚ñ≤ = Xin V‚ñ≤ V‚ñ≤T para reconstruir el input pero truncado
        #xpdot = torch.mm(kernel,torch.t(kernel)) # es el producto interno de kernel, es decir del code
        #x = torch.mm(xpdot.float(),xin)  # es el x‚ñ≤
        
        #X‚ñ≤ =  V‚ñ≤T V‚ñ≤ Xin para reconstruir el input pero truncado
        xpdot = np.dot(Vtrunc.transpose(), Vtrunc) # con la nueva despejada
        x = np.dot(xpdot, xin)
        x = torch.from_numpy(x).to(device)
        x = x.float()
       
        x = torch.unsqueeze(x, 2)
        x = torch.unsqueeze(x, 3)
        x = F.relu(self.norm3(self.dconv4(x)))
        x = F.relu(self.norm3(self.conv8(x)))
        x = F.relu(self.norm2(self.dconv1(x)))
        x = F.relu(self.norm2(self.conv9(x)))
        x = F.relu(self.norm1(self.dconv2(x)))
        x = F.relu(self.norm1(self.conv10(x)))
        x = self.norm4(self.dconv3(x))
        x = torch.tanh(x)

        print("DecodeRd aplicado",x.shape)

        return x

#autoenc = AutoEncoder(codesize=2).apply(weights_init)
#autoenc.to(device)

########################################################################
# 2. Define a Convolution Neural Network 32*32
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch

class AutoEncoder32(nn.Module):
    
    def __init__(self, codesize, nChanelsIn):
        super().__init__()
        
        # Encoder specification
        self.pool = nn.MaxPool2d(2, 2)
        self.conv1 = nn.Conv2d(nChanelsIn, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, codesize, 3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv6 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv7 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        
        #-----------------------------My encoder specification
        self.minorm16 = nn.BatchNorm2d(16, momentum=1, affine=True)
        self.miconv16 = nn.Conv2d(32, 16, 3, padding=1)
        self.miconv1616 = nn.Conv2d(16, 16, 3, padding=1)
        self.miconvd = nn.Conv2d(16, codesize, 3, padding=1) # El 16 creo que toca hacerlo variable para que se adapte a 32 (si es 28*28) o 16 (si es 32*32)
        
        
        # Decoder specification
        self.dconv1 = nn.ConvTranspose2d(codesize, 32, 3, stride=2)
        self.dconv2 = nn.ConvTranspose2d(32, 64, 2, stride=2)
        self.dconv3 = nn.ConvTranspose2d(64, 1, 2, stride=2)
        self.dconv4 = nn.ConvTranspose2d(codesize, codesize, 2, stride=2)
        self.conv8 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv9 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv10 = nn.Conv2d(64, 64, 3, padding=1)
        
        #-----------------------------My encoder specification
        self.midconv16 = nn.ConvTranspose2d(codesize, 16, 2, stride=2) # in_channels, out_channels, kernel_size, stride, padding
        self.midconv32 = nn.ConvTranspose2d(16, 32, 2, stride=2)
        
        
    def forward(self, x):
        code = self.encode(x)
        out = self.decode(code)
        return out, code
    
    def encode(self, x):
        #print("x", x.shape) # [64, 1, 32, 32]
        x = F.relu(self.norm1(self.conv1(x))) # 1,64, 3,1
        #print("conv 1", x.shape) # [64, 64, 32, 32]
        x = F.relu(self.norm1(self.conv4(x))) # 64,64, 3,1
        #print("conv 4", x.shape) # [64, 64, 32, 32]
        x = self.pool(x) #2,2
        #print("pool", x.shape) #[64, 64, 16, 16]
        x = F.relu(self.norm2(self.conv2(x))) # 64,32, 3,1
        #print("conv 2", x.shape) # [64, 32, 16, 16]
        x = F.relu(self.norm2(self.conv5(x))) # 32,32, 3,1
        #print("conv 5", x.shape) # [64, 32, 16, 16]
        x = self.pool(x) 
        #print("pool", x.shape) # [64, 32, 8, 8]

        #----32*32  creo que aqui toca poner un if dependiendo si la imagen es 32*32 pone este bloque
        x = F.relu(self.minorm16(self.miconv16(x))) # 32 canales, 16 kernels, 3 strid, 1 padding
        #print("conv16",x.shape) #conv1616 torch.Size([64, 16, 8, 8])
        x = F.relu(self.minorm16(self.miconv1616(x))) # 16 canales, 16 kernels, 3,1
        #print("conv1616",x.shape) #conv1616 torch.Size([64, 16, 8, 8])
        x = self.pool(x) 
        #print("mi pool 3",x.shape) #mi pool 3 torch.Size([64, 16, 4, 4])
        x = F.relu(self.norm3(self.miconvd(x))) # 16,d, 3,1
        #print("miconvd", x.shape) # [64, 2, 4, 4]
        #----
        
        #x = F.relu(self.norm3(self.conv3(x))) # remplac√© la conv 3 (32,codsize) por miconvd (16,codsize) .... (creo que arriba micovd debe ser variable)
        #print("conv3", x.shape)
        
        x = F.relu(self.norm3(self.conv6(x))) # d,d, 3,1
        #print("conv 6", x.shape) # [64, 2, 4, 4]
        x = self.pool(x)
        #print("pool", x.shape) # [64, 2, 2, 2]
        x = torch.sigmoid(self.norm3(self.conv7(x))) # d,d, 3,1
        #print("conv 7", x.shape) # [64, 2, 2, 2]
        x = self.pool(x)
        #print("pool", x.shape) # [64, 2, 1, 1]
        x = torch.squeeze(x)
        #print("squeeze", x.shape) # [64, 2]
        return x
    
    def decode(self, x):
        #print("Decode...")
        #print("x d", x.shape) # [64, 2]
        x = torch.unsqueeze(x, 2)
        #print("unsqueeze", x.shape) # [64, 2, 1]
        x = torch.unsqueeze(x, 3)
        #print("unsqueeze", x.shape) # [64, 2, 1, 1]
        
        x = F.relu(self.norm3(self.dconv4(x))) # d,d, 2,2
        #print("d conv4", x.shape) # [64, 2, 3, 3]
        x = F.relu(self.norm3(self.conv8(x))) # d,d, 3,1
        #print("conv8", x.shape) # [64, 2, 3, 3]

        #----32*32
        x = F.relu(self.minorm16(self.midconv16(x))) # d,16, 2, 2
        #print("midcov16", x.shape)
        x = F.relu(self.minorm16(self.miconv1616(x))) # 16,16, 3,1
        #print("miconv1616", x.shape)
        x = F.relu(self.norm2(self.midconv32(x))) # 16,32, 2,2
        #print("midconv32", x.shape)
        #----
        
        #x = F.relu(self.norm2(self.dconv1(x))) # d,32, 3,2 remplace la dconv1 por  midconv1
        #print("d conv1", x.shape) # [64, 32, 7, 7]
        
        x = F.relu(self.norm2(self.conv9(x))) # 32,32, 3,1
        #print("conv9", x.shape) # [64, 32, 7, 7]
        x = F.relu(self.norm1(self.dconv2(x))) # 32,64, 2,2
        #print("d conv2", x.shape) # [64, 64, 14, 14]
        x = F.relu(self.norm1(self.conv10(x))) # 64,64, 3,1
        #print("conv10", x.shape) # [64, 64, 14, 14]
        x = self.norm4(self.dconv3(x))  # 64,1, 2,2
        #print("d conv3", x.shape) # [64, 1, 28, 28]
        
        x = torch.tanh(x) 
        #print("tanh", x.shape) # [64, 1, 32, 32]
        
        return x
    
    def decodeRd(self, kernel, xin):

        #X‚ñ≤ =  V‚ñ≤T V‚ñ≤ Xin para reconstruir el input pero truncado
        xpdot = np.dot(Vtrunc.transpose(), Vtrunc) # con la nueva despejada
        x = np.dot(xpdot, xin)
        x = torch.from_numpy(x).to(device)
        x = x.float()
        
        x = torch.unsqueeze(x, 2)
        #print("unsqueeze", x.shape) # [64, 2, 1]
        x = torch.unsqueeze(x, 3)
        #print("unsqueeze", x.shape) # [64, 2, 1, 1]
        
        x = F.relu(self.norm3(self.dconv4(x))) # d,d, 2,2
        #print("d conv4", x.shape) # [64, 2, 3, 3]
        x = F.relu(self.norm3(self.conv8(x))) # d,d, 3,1
        #print("conv8", x.shape) # [64, 2, 3, 3]

        #----32*32
        x = F.relu(self.minorm16(self.midconv16(x))) # d,16, 2, 2
        #print("midcov16", x.shape)
        x = F.relu(self.minorm16(self.miconv1616(x))) # 16,16, 3,1
        #print("miconv1616", x.shape)
        x = F.relu(self.norm2(self.midconv32(x))) # 16,32, 2,2
        #print("midconv32", x.shape)
        #----
        
        #x = F.relu(self.norm2(self.dconv1(x))) # d,32, 3,2 remplace la dconv1 por  midconv1
        #print("d conv1", x.shape) # [64, 32, 7, 7]
        
        x = F.relu(self.norm2(self.conv9(x))) # 32,32, 3,1
        #print("conv9", x.shape) # [64, 32, 7, 7]
        x = F.relu(self.norm1(self.dconv2(x))) # 32,64, 2,2
        #print("d conv2", x.shape) # [64, 64, 14, 14]
        x = F.relu(self.norm1(self.conv10(x))) # 64,64, 3,1
        #print("conv10", x.shape) # [64, 64, 14, 14]
        x = self.norm4(self.dconv3(x))  # 64,1, 2,2
        #print("d conv3", x.shape) # [64, 1, 28, 28]
        
        x = torch.tanh(x) 
        #print("tanh", x.shape) # [64, 1, 28, 28]
        
        #print("DecodeRd aplicado",x.shape)

        return x
    
#autoenc = AutoEncoder(codesize=2).apply(weights_init)
#autoenc.to(device)

########################################################################
# 2. Define a Convolution Neural Network 28*20
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch

class AutoEncoder2820(nn.Module):
    
    def __init__(self, codesize):
        super().__init__()
        
        # Encoder specification
        self.pool = nn.MaxPool2d(2, 2)
        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, codesize, 3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv6 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv7 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        
        # Decoder specification
        self.dconv1 = nn.ConvTranspose2d(codesize, 32, 3, stride=2) 
        
        self.dconv2 = nn.ConvTranspose2d(32, 64, 2, stride=2)
        self.dconv3 = nn.ConvTranspose2d(64, 1, 2, stride=2)
        self.dconv4 = nn.ConvTranspose2d(codesize, codesize, kernel_size=(3, 2), stride=2)
        self.conv8 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv9 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv10 = nn.Conv2d(64, 64, 3, padding=1)

    def forward(self, x):
        code = self.encode(x)
        out = self.decode(code)
        return out, code
    
    def encode(self, x):
        #print("Encode...")
        #print("x", x.shape)
        x = F.relu(self.norm1(self.conv1(x)))
        #print("conv 1", x.shape)
        x = F.relu(self.norm1(self.conv4(x)))
        #print("conv 4", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm2(self.conv2(x)))
        #print("conv 2", x.shape)
        x = F.relu(self.norm2(self.conv5(x)))
        #print("conv 5", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm3(self.conv3(x)))
        #print("conv 3", x.shape)
        x = F.relu(self.norm3(self.conv6(x)))
        #print("conv 6", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.sigmoid(self.norm3(self.conv7(x)))
        #print("conv 7", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.squeeze(x)
        #print("squeeze", x.shape)
        return x
    
    def decode(self, x):
        #print("Decode...")
        #print("xin", x.shape)
        x = torch.unsqueeze(x, 2)
        #print("unsqueeze", x.shape)
        x = torch.unsqueeze(x, 3)
        #print("unsqueeze", x.shape)
        x = F.relu(self.norm3(self.dconv4(x)))
        #print("dconv4", x.shape)
        x = F.relu(self.norm3(self.conv8(x)))
        #print("conv8", x.shape)
        x = F.relu(self.norm2(self.dconv1(x)))
        #print("dconv1", x.shape)
        x = F.relu(self.norm2(self.conv9(x)))
        #print("conv9", x.shape)
        x = F.relu(self.norm1(self.dconv2(x)))
        #print("dconv2", x.shape)
        x = F.relu(self.norm1(self.conv10(x)))
        #print("conv10", x.shape)
        x = self.norm4(self.dconv3(x))
        #print("dconv3", x.shape)
        x = torch.tanh(x)
        #print("tanh", x.shape)
        return x
    
    def decodeRd(self, kernel, xin):
        
        #X‚ñ≤ =  V‚ñ≤T V‚ñ≤ Xin para reconstruir el input pero truncado
        xpdot = np.dot(Vtrunc.transpose(), Vtrunc) # con la nueva despejada
        x = np.dot(xpdot, xin)
        x = torch.from_numpy(x).to(device)
        x = x.float()
       
        x = torch.unsqueeze(x, 2)
        x = torch.unsqueeze(x, 3)
        x = F.relu(self.norm3(self.dconv4(x)))
        x = F.relu(self.norm3(self.conv8(x)))
        x = F.relu(self.norm2(self.dconv1(x)))
        x = F.relu(self.norm2(self.conv9(x)))
        x = F.relu(self.norm1(self.dconv2(x)))
        x = F.relu(self.norm1(self.conv10(x)))
        x = self.norm4(self.dconv3(x))
        x = torch.tanh(x)

        print("DecodeRd aplicado",x.shape)

        return x

import math

def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
        m.weight.data.normal_(0, math.sqrt(2. / n))
        if m.bias is not None:
            m.bias.data.zero_()
    elif classname.find('BatchNorm') != -1:
        m.weight.data.fill_(1)
        m.bias.data.zero_()
    elif classname.find('Linear') != -1:
        n = m.weight.size(1)
        m.weight.data.normal_(0, 0.01)
        m.bias.data = torch.ones(m.bias.data.size())

########################################################################
# AUTOENCODERS PARA IN
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

########################################################################
# Define a Convolution Neural Network 28*28 RD IN
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch
import numpy as np

# from KPCA import kpca
# from LLE import lle,klleGpu
# from MDS import kcmdsGpu
# from LE import kleGpu

class AutoEncoderIN28(nn.Module):
    
    def __init__(self, codesize, selMetRd):
        super().__init__()
        
        self.selMetRd = selMetRd
        
        # Encoder specification
        self.pool = nn.MaxPool2d(2, 2)
        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, codesize, 3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv6 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv7 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        
        # Decoder specification
        self.dconv1 = nn.ConvTranspose2d(codesize, 32, 3, stride=2)
        self.dconv2 = nn.ConvTranspose2d(32, 64, 2, stride=2)
        self.dconv3 = nn.ConvTranspose2d(64, 1, 2, stride=2)
        self.dconv4 = nn.ConvTranspose2d(codesize, codesize, 3, stride=2)
        self.conv8 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv9 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv10 = nn.Conv2d(64, 64, 3, padding=1)

    def forward(self, x):
        code, Vtrunc, xin = self.encode(x)
        out = self.decode(Vtrunc, xin)
        return out, code
    
    def encode(self, x):
        #print("x", x.shape)
        x = F.relu(self.norm1(self.conv1(x)))
        #print("conv 1", x.shape)
        x = F.relu(self.norm1(self.conv4(x)))
        #print("conv 4", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm2(self.conv2(x)))
        #print("conv 2", x.shape)
        x = F.relu(self.norm2(self.conv5(x)))
        #print("conv 5", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm3(self.conv3(x)))
        #print("conv 3", x.shape)
        x = F.relu(self.norm3(self.conv6(x)))
        #print("conv 6", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.sigmoid(self.norm3(self.conv7(x)))
        #print("conv 7", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.squeeze(x)
        #print("squeeze", x.shape)
        
        #______ APLICACI√ìN RD IN
        xin = x
        print("el metodo rd seleccionado fue: ", self.selMetRd)
        if self.selMetRd=="indCMDS":
            x, Vtrunc = kcmdsGpu(x, 2)
            
        if self.selMetRd=="indLLE":
            x, Vtrunc = klleGpu(x, 38, 2)
            
        if self.selMetRd=="indLE":
            x, Vtrunc = kleGpu(x, 38, 2)
            
        if self.selMetRd=="comLineal":
            k1, v1 = kcmdsGpu(encodeRd, 2)
            k2, v2 =kleGpu(encodeRd, 38, 2) # k = 15
            k3, v3 =klleGpu(encodeRd, 38, 2) # k = 100
            x = 0.3*k1 + 0.3*k2 + 0.4*k3
            Vtrunc = 0.3*v1 + 0.3*v2 + 0.4*v3
            
        if self.selMetRd=="deepRd":
            x, Vtrunc = kleGpu(x, 38, 2) # k = 15
            x, Vtrunc = klleGpu(x, 38, 2) # = 100
            x, Vtrunc = kcmdsGpu(x, 2)
        #______
             
        return x, Vtrunc, xin
    
    def decode(self, Vtrunc, xin):
        xin = xin.cpu().detach().numpy()
        #X‚ñ≤ =  V‚ñ≤T V‚ñ≤ Xin para reconstruir el input pero truncado
        xpdot = np.dot(Vtrunc.transpose(), Vtrunc) # con la nueva despejada
        x = np.dot(xpdot, xin)
        x = torch.from_numpy(x).to(device)
        x = x.float()
       
        x = torch.unsqueeze(x, 2)
        x = torch.unsqueeze(x, 3)
        x = F.relu(self.norm3(self.dconv4(x)))
        x = F.relu(self.norm3(self.conv8(x)))
        x = F.relu(self.norm2(self.dconv1(x)))
        x = F.relu(self.norm2(self.conv9(x)))
        x = F.relu(self.norm1(self.dconv2(x)))
        x = F.relu(self.norm1(self.conv10(x)))
        x = self.norm4(self.dconv3(x))
        x = torch.tanh(x)

        print("DecodeRd aplicado",x.shape)

        return x

#autoenc = AutoEncoder(codesize=2).apply(weights_init)
#autoenc.to(device)

########################################################################
# 2. Define a Convolution Neural Network 32*32
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch

class AutoEncoderIN32(nn.Module):
    
    def __init__(self, codesize, nChanelsIn, selMetRd):
        super().__init__()
        
        self.selMetRd = selMetRd
        
        # Encoder specification
        self.pool = nn.MaxPool2d(2, 2)
        self.conv1 = nn.Conv2d(nChanelsIn, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, codesize, 3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv6 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv7 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        
        #-----------------------------My encoder specification
        self.minorm16 = nn.BatchNorm2d(16, momentum=1, affine=True)
        self.miconv16 = nn.Conv2d(32, 16, 3, padding=1)
        self.miconv1616 = nn.Conv2d(16, 16, 3, padding=1)
        self.miconvd = nn.Conv2d(16, codesize, 3, padding=1) # El 16 creo que toca hacerlo variable para que se adapte a 32 (si es 28*28) o 16 (si es 32*32)
        
        
        # Decoder specification
        self.dconv1 = nn.ConvTranspose2d(codesize, 32, 3, stride=2)
        self.dconv2 = nn.ConvTranspose2d(32, 64, 2, stride=2)
        self.dconv3 = nn.ConvTranspose2d(64, 1, 2, stride=2)
        self.dconv4 = nn.ConvTranspose2d(codesize, codesize, 2, stride=2)
        self.conv8 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv9 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv10 = nn.Conv2d(64, 64, 3, padding=1)
        
        #-----------------------------My encoder specification
        self.midconv16 = nn.ConvTranspose2d(codesize, 16, 2, stride=2) # in_channels, out_channels, kernel_size, stride, padding
        self.midconv32 = nn.ConvTranspose2d(16, 32, 2, stride=2)
        
    
    def forward(self, x):
        code, Vtrunc, xin = self.encode(x)
        out = self.decode(Vtrunc, xin)
        return out, code
    
    def encode(self, x):
        #print("x", x.shape) # [64, 1, 32, 32]
        x = F.relu(self.norm1(self.conv1(x))) # 1,64, 3,1
        #print("conv 1", x.shape) # [64, 64, 32, 32]
        x = F.relu(self.norm1(self.conv4(x))) # 64,64, 3,1
        #print("conv 4", x.shape) # [64, 64, 32, 32]
        x = self.pool(x) #2,2
        #print("pool", x.shape) #[64, 64, 16, 16]
        x = F.relu(self.norm2(self.conv2(x))) # 64,32, 3,1
        #print("conv 2", x.shape) # [64, 32, 16, 16]
        x = F.relu(self.norm2(self.conv5(x))) # 32,32, 3,1
        #print("conv 5", x.shape) # [64, 32, 16, 16]
        x = self.pool(x) 
        #print("pool", x.shape) # [64, 32, 8, 8]

        #----32*32  creo que aqui toca poner un if dependiendo si la imagen es 32*32 pone este bloque
        x = F.relu(self.minorm16(self.miconv16(x))) # 32 canales, 16 kernels, 3 strid, 1 padding
        #print("conv16",x.shape) #conv1616 torch.Size([64, 16, 8, 8])
        x = F.relu(self.minorm16(self.miconv1616(x))) # 16 canales, 16 kernels, 3,1
        #print("conv1616",x.shape) #conv1616 torch.Size([64, 16, 8, 8])
        x = self.pool(x) 
        #print("mi pool 3",x.shape) #mi pool 3 torch.Size([64, 16, 4, 4])
        x = F.relu(self.norm3(self.miconvd(x))) # 16,d, 3,1
        #print("miconvd", x.shape) # [64, 2, 4, 4]
        #----
        
        #x = F.relu(self.norm3(self.conv3(x))) # remplac√© la conv 3 (32,codsize) por miconvd (16,codsize) .... (creo que arriba micovd debe ser variable)
        #print("conv3", x.shape)
        
        x = F.relu(self.norm3(self.conv6(x))) # d,d, 3,1
        #print("conv 6", x.shape) # [64, 2, 4, 4]
        x = self.pool(x)
        #print("pool", x.shape) # [64, 2, 2, 2]
        x = torch.sigmoid(self.norm3(self.conv7(x))) # d,d, 3,1
        #print("conv 7", x.shape) # [64, 2, 2, 2]
        x = self.pool(x)
        #print("pool", x.shape) # [64, 2, 1, 1]
        x = torch.squeeze(x)
        #print("squeeze", x.shape) # [64, 2]
        
        #______ APLICACI√ìN RD IN
        xin = x
        print("el metodo rd seleccionado fue: ", self.selMetRd)
        if self.selMetRd=="indCMDS":
            x, Vtrunc = kcmdsGpu(x, 2)
            
        if self.selMetRd=="indLLE":
            x, Vtrunc = klleGpu(x, 38, 2)
            
        if self.selMetRd=="indLE":
            x, Vtrunc = kleGpu(x, 38, 2)
            
        if self.selMetRd=="comLineal":
            k1, v1 = kcmdsGpu(encodeRd, 2)
            k2, v2 =kleGpu(encodeRd, 38, 2) # k = 15
            k3, v3 =klleGpu(encodeRd, 38, 2) # k = 100
            x = 0.3*k1 + 0.3*k2 + 0.4*k3
            Vtrunc = 0.3*v1 + 0.3*v2 + 0.4*v3
            
        if self.selMetRd=="deepRd":
            x, Vtrunc = kleGpu(x, 38, 2) # k = 15
            x, Vtrunc = klleGpu(x, 38, 2) # = 100
            x, Vtrunc = kcmdsGpu(x, 2)
        #______
        
        return x, Vtrunc, xin
    
    def decode(self, Vtrunc, xin):
        
        xin = xin.cpu().detach().numpy()
        #X‚ñ≤ =  V‚ñ≤T V‚ñ≤ Xin para reconstruir el input pero truncado
        xpdot = np.dot(Vtrunc.transpose(), Vtrunc) # con la nueva despejada
        x = np.dot(xpdot, xin)
        x = torch.from_numpy(x).to(device)
        x = x.float()
        
        #print("Decode...")
        #print("x d", x.shape) # [64, 2]
        x = torch.unsqueeze(x, 2)
        #print("unsqueeze", x.shape) # [64, 2, 1]
        x = torch.unsqueeze(x, 3)
        #print("unsqueeze", x.shape) # [64, 2, 1, 1]
        
        x = F.relu(self.norm3(self.dconv4(x))) # d,d, 2,2
        #print("d conv4", x.shape) # [64, 2, 3, 3]
        x = F.relu(self.norm3(self.conv8(x))) # d,d, 3,1
        #print("conv8", x.shape) # [64, 2, 3, 3]

        #----32*32
        x = F.relu(self.minorm16(self.midconv16(x))) # d,16, 2, 2
        #print("midcov16", x.shape)
        x = F.relu(self.minorm16(self.miconv1616(x))) # 16,16, 3,1
        #print("miconv1616", x.shape)
        x = F.relu(self.norm2(self.midconv32(x))) # 16,32, 2,2
        #print("midconv32", x.shape)
        #----
        
        #x = F.relu(self.norm2(self.dconv1(x))) # d,32, 3,2 remplace la dconv1 por  midconv1
        #print("d conv1", x.shape) # [64, 32, 7, 7]
        
        x = F.relu(self.norm2(self.conv9(x))) # 32,32, 3,1
        #print("conv9", x.shape) # [64, 32, 7, 7]
        x = F.relu(self.norm1(self.dconv2(x))) # 32,64, 2,2
        #print("d conv2", x.shape) # [64, 64, 14, 14]
        x = F.relu(self.norm1(self.conv10(x))) # 64,64, 3,1
        #print("conv10", x.shape) # [64, 64, 14, 14]
        x = self.norm4(self.dconv3(x))  # 64,1, 2,2
        #print("d conv3", x.shape) # [64, 1, 28, 28]
        
        x = torch.tanh(x) 
        #print("tanh", x.shape) # [64, 1, 32, 32]
        
        return x

########################################################################
# 2. Define a Convolution Neural Network 28*20
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch

class AutoEncoderIN2820(nn.Module):
    
    def __init__(self, codesize, selMetRd):
        super().__init__()
        
        self.selMetRd = selMetRd
        
        # Encoder specification
        self.pool = nn.MaxPool2d(2, 2)
        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, codesize, 3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv6 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv7 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        
        # Decoder specification
        self.dconv1 = nn.ConvTranspose2d(codesize, 32, 3, stride=2) 
        
        self.dconv2 = nn.ConvTranspose2d(32, 64, 2, stride=2)
        self.dconv3 = nn.ConvTranspose2d(64, 1, 2, stride=2)
        self.dconv4 = nn.ConvTranspose2d(codesize, codesize, kernel_size=(3, 2), stride=2)
        self.conv8 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv9 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv10 = nn.Conv2d(64, 64, 3, padding=1)

    def forward(self, x):
        code, Vtrunc, xin = self.encode(x)
        out = self.decode(Vtrunc, xin)
        return out, code
    
    def encode(self, x):
        #print("Encode...")
        #print("x", x.shape)
        x = F.relu(self.norm1(self.conv1(x)))
        #print("conv 1", x.shape)
        x = F.relu(self.norm1(self.conv4(x)))
        #print("conv 4", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm2(self.conv2(x)))
        #print("conv 2", x.shape)
        x = F.relu(self.norm2(self.conv5(x)))
        #print("conv 5", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm3(self.conv3(x)))
        #print("conv 3", x.shape)
        x = F.relu(self.norm3(self.conv6(x)))
        #print("conv 6", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.sigmoid(self.norm3(self.conv7(x)))
        #print("conv 7", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.squeeze(x)
        #print("squeeze", x.shape)
        
        #______ APLICACI√ìN RD IN
        xin = x
        print("el metodo rd seleccionado fue: ", self.selMetRd)
        if self.selMetRd=="indCMDS":
            x, Vtrunc = kcmdsGpu(x, 2)
            
        if self.selMetRd=="indLLE":
            x, Vtrunc = klleGpu(x, 38, 2)
            
        if self.selMetRd=="indLE":
            x, Vtrunc = kleGpu(x, 38, 2)
            
        if self.selMetRd=="comLineal":
            k1, v1 = kcmdsGpu(encodeRd, 2)
            k2, v2 =kleGpu(encodeRd, 38, 2) # k = 15
            k3, v3 =klleGpu(encodeRd, 38, 2) # k = 100
            x = 0.3*k1 + 0.3*k2 + 0.4*k3
            Vtrunc = 0.3*v1 + 0.3*v2 + 0.4*v3
            
        if self.selMetRd=="deepRd":
            x, Vtrunc = kleGpu(x, 38, 2) # k = 15
            x, Vtrunc = klleGpu(x, 38, 2) # = 100
            x, Vtrunc = kcmdsGpu(x, 2)
        #______
        
        return x, Vtrunc, xin
    
    def decode(self, Vtrunc, xin):
        
        xin = xin.cpu().detach().numpy()
        #X‚ñ≤ =  V‚ñ≤T V‚ñ≤ Xin para reconstruir el input pero truncado
        xpdot = np.dot(Vtrunc.transpose(), Vtrunc) # con la nueva despejada
        x = np.dot(xpdot, xin)
        x = torch.from_numpy(x).to(device)
        x = x.float()
        
        #print("Decode...")
        #print("xin", x.shape)
        x = torch.unsqueeze(x, 2)
        #print("unsqueeze", x.shape)
        x = torch.unsqueeze(x, 3)
        #print("unsqueeze", x.shape)
        x = F.relu(self.norm3(self.dconv4(x)))
        #print("dconv4", x.shape)
        x = F.relu(self.norm3(self.conv8(x)))
        #print("conv8", x.shape)
        x = F.relu(self.norm2(self.dconv1(x)))
        #print("dconv1", x.shape)
        x = F.relu(self.norm2(self.conv9(x)))
        #print("conv9", x.shape)
        x = F.relu(self.norm1(self.dconv2(x)))
        #print("dconv2", x.shape)
        x = F.relu(self.norm1(self.conv10(x)))
        #print("conv10", x.shape)
        x = self.norm4(self.dconv3(x))
        #print("dconv3", x.shape)
        x = torch.tanh(x)
        #print("tanh", x.shape)
        
        return x

########################################################################
# AUTOENCODERS PARA PRE-IN
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

########################################################################
# Define a Convolution Neural Network 28*28 RD PRE + IN
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch
import numpy as np

# from KPCA import kpca
# from LLE import lle,klleGpu
# from MDS import kcmdsGpu
# from LE import kleGpu

class AutoEncoderPreIn28(nn.Module):
    
    def __init__(self, codesize, krd):
        super().__init__()
        
        self.krd = krd
        
        # Encoder specification
        self.pool = nn.MaxPool2d(2, 2)
        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, codesize, 3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv6 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv7 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        
        # Decoder specification
        self.dconv1 = nn.ConvTranspose2d(codesize, 32, 3, stride=2)
        self.dconv2 = nn.ConvTranspose2d(32, 64, 2, stride=2)
        self.dconv3 = nn.ConvTranspose2d(64, 1, 2, stride=2)
        self.dconv4 = nn.ConvTranspose2d(codesize, codesize, 3, stride=2)
        self.conv8 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv9 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv10 = nn.Conv2d(64, 64, 3, padding=1)

    def forward(self, x):
        
        code = self.encode(x)
        out = self.decode(code)
        return out, code
    
    def encode(self, x):
        #print("x", x.shape)
        x = F.relu(self.norm1(self.conv1(x)))
        #print("conv 1", x.shape)
        x = F.relu(self.norm1(self.conv4(x)))
        #print("conv 4", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm2(self.conv2(x)))
        #print("conv 2", x.shape)
        x = F.relu(self.norm2(self.conv5(x)))
        #print("conv 5", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm3(self.conv3(x)))
        #print("conv 3", x.shape)
        x = F.relu(self.norm3(self.conv6(x)))
        #print("conv 6", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.sigmoid(self.norm3(self.conv7(x)))
        #print("conv 7", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.squeeze(x)
        #print("squeeze", x.shape)
        
        # Aplica la base de la RD
        x = torch.mm(x,self.krd)   
             
        return x
    
    def decode(self, x):
        
        #X‚ñ≤ =  V‚ñ≤T V‚ñ≤ Xin para reconstruir el input pero truncado
        x = torch.mm(x,torch.t(self.krd)) 
       
        x = torch.unsqueeze(x, 2)
        x = torch.unsqueeze(x, 3)
        x = F.relu(self.norm3(self.dconv4(x)))
        x = F.relu(self.norm3(self.conv8(x)))
        x = F.relu(self.norm2(self.dconv1(x)))
        x = F.relu(self.norm2(self.conv9(x)))
        x = F.relu(self.norm1(self.dconv2(x)))
        x = F.relu(self.norm1(self.conv10(x)))
        x = self.norm4(self.dconv3(x))
        x = torch.tanh(x)

        #print("DecodeRd aplicado",x.shape)

        return x
    
 #autoenc = AutoEncoder(codesize=2).apply(weights_init)
#autoenc.to(device)

########################################################################
# 2. Define a Convolution Neural Network 32*32
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch

class AutoEncoderPreIn32(nn.Module):
    
    def __init__(self, codesize, nChanelsIn, krd):
        super().__init__()
        
        self.krd = krd      
        
        # Encoder specification
        self.pool = nn.MaxPool2d(2, 2)
        self.conv1 = nn.Conv2d(nChanelsIn, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, codesize, 3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv6 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv7 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        
        #-----------------------------My encoder specification
        self.minorm16 = nn.BatchNorm2d(16, momentum=1, affine=True)
        self.miconv16 = nn.Conv2d(32, 16, 3, padding=1)
        self.miconv1616 = nn.Conv2d(16, 16, 3, padding=1)
        self.miconvd = nn.Conv2d(16, codesize, 3, padding=1) # El 16 creo que toca hacerlo variable para que se adapte a 32 (si es 28*28) o 16 (si es 32*32)
        
        
        # Decoder specification
        self.dconv1 = nn.ConvTranspose2d(codesize, 32, 3, stride=2)
        self.dconv2 = nn.ConvTranspose2d(32, 64, 2, stride=2)
        self.dconv3 = nn.ConvTranspose2d(64, 1, 2, stride=2)
        self.dconv4 = nn.ConvTranspose2d(codesize, codesize, 2, stride=2)
        self.conv8 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv9 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv10 = nn.Conv2d(64, 64, 3, padding=1)
        
        #-----------------------------My encoder specification
        self.midconv16 = nn.ConvTranspose2d(codesize, 16, 2, stride=2) # in_channels, out_channels, kernel_size, stride, padding
        self.midconv32 = nn.ConvTranspose2d(16, 32, 2, stride=2)     
        
    def forward(self, x):
        code = self.encode(x)
        out = self.decode(code)
        return out, code
    
    def encode(self, x):
        #print("x", x.shape) # [64, 1, 32, 32]
        x = F.relu(self.norm1(self.conv1(x))) # 1,64, 3,1
        #print("conv 1", x.shape) # [64, 64, 32, 32]
        x = F.relu(self.norm1(self.conv4(x))) # 64,64, 3,1
        #print("conv 4", x.shape) # [64, 64, 32, 32]
        x = self.pool(x) #2,2
        #print("pool", x.shape) #[64, 64, 16, 16]
        x = F.relu(self.norm2(self.conv2(x))) # 64,32, 3,1
        #print("conv 2", x.shape) # [64, 32, 16, 16]
        x = F.relu(self.norm2(self.conv5(x))) # 32,32, 3,1
        #print("conv 5", x.shape) # [64, 32, 16, 16]
        x = self.pool(x) 
        #print("pool", x.shape) # [64, 32, 8, 8]

        #----32*32  creo que aqui toca poner un if dependiendo si la imagen es 32*32 pone este bloque
        x = F.relu(self.minorm16(self.miconv16(x))) # 32 canales, 16 kernels, 3 strid, 1 padding
        #print("conv16",x.shape) #conv1616 torch.Size([64, 16, 8, 8])
        x = F.relu(self.minorm16(self.miconv1616(x))) # 16 canales, 16 kernels, 3,1
        #print("conv1616",x.shape) #conv1616 torch.Size([64, 16, 8, 8])
        x = self.pool(x) 
        #print("mi pool 3",x.shape) #mi pool 3 torch.Size([64, 16, 4, 4])
        x = F.relu(self.norm3(self.miconvd(x))) # 16,d, 3,1
        #print("miconvd", x.shape) # [64, 2, 4, 4]
        #----
        
        #x = F.relu(self.norm3(self.conv3(x))) # remplac√© la conv 3 (32,codsize) por miconvd (16,codsize) .... (creo que arriba micovd debe ser variable)
        #print("conv3", x.shape)
        
        x = F.relu(self.norm3(self.conv6(x))) # d,d, 3,1
        #print("conv 6", x.shape) # [64, 2, 4, 4]
        x = self.pool(x)
        #print("pool", x.shape) # [64, 2, 2, 2]
        x = torch.sigmoid(self.norm3(self.conv7(x))) # d,d, 3,1
        #print("conv 7", x.shape) # [64, 2, 2, 2]
        x = self.pool(x)
        #print("pool", x.shape) # [64, 2, 1, 1]
        x = torch.squeeze(x)
        #print("squeeze", x.shape) # [64, 2]
        
        # Aplica la base de la RD
        x = torch.mm(x,self.krd)   
        
        return x
    
    def decode(self, x):
        
        #X‚ñ≤ =  V‚ñ≤T V‚ñ≤ Xin para reconstruir el input pero truncado
        x = torch.mm(x,torch.t(self.krd)) 
        
        #print("Decode...")
        #print("x d", x.shape) # [64, 2]
        x = torch.unsqueeze(x, 2)
        #print("unsqueeze", x.shape) # [64, 2, 1]
        x = torch.unsqueeze(x, 3)
        #print("unsqueeze", x.shape) # [64, 2, 1, 1]
        
        x = F.relu(self.norm3(self.dconv4(x))) # d,d, 2,2
        #print("d conv4", x.shape) # [64, 2, 3, 3]
        x = F.relu(self.norm3(self.conv8(x))) # d,d, 3,1
        #print("conv8", x.shape) # [64, 2, 3, 3]

        #----32*32
        x = F.relu(self.minorm16(self.midconv16(x))) # d,16, 2, 2
        #print("midcov16", x.shape)
        x = F.relu(self.minorm16(self.miconv1616(x))) # 16,16, 3,1
        #print("miconv1616", x.shape)
        x = F.relu(self.norm2(self.midconv32(x))) # 16,32, 2,2
        #print("midconv32", x.shape)
        #----
        
        #x = F.relu(self.norm2(self.dconv1(x))) # d,32, 3,2 remplace la dconv1 por  midconv1
        #print("d conv1", x.shape) # [64, 32, 7, 7]
        
        x = F.relu(self.norm2(self.conv9(x))) # 32,32, 3,1
        #print("conv9", x.shape) # [64, 32, 7, 7]
        x = F.relu(self.norm1(self.dconv2(x))) # 32,64, 2,2
        #print("d conv2", x.shape) # [64, 64, 14, 14]
        x = F.relu(self.norm1(self.conv10(x))) # 64,64, 3,1
        #print("conv10", x.shape) # [64, 64, 14, 14]
        x = self.norm4(self.dconv3(x))  # 64,1, 2,2
        #print("d conv3", x.shape) # [64, 1, 28, 28]
        
        x = torch.tanh(x) 
        #print("tanh", x.shape) # [64, 1, 32, 32]
        
        return x

########################################################################
# 2. Define a Convolution Neural Network 28*20
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch

class AutoEncoderPreIn2820(nn.Module):
    
    def __init__(self, codesize, krd):
        super().__init__()
        
        self.krd = krd      
                
        # Encoder specification
        self.pool = nn.MaxPool2d(2, 2)
        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, codesize, 3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv6 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv7 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.norm1 = nn.BatchNorm2d(64, momentum=1, affine=True)
        self.norm2 = nn.BatchNorm2d(32, momentum=1, affine=True)
        self.norm3 = nn.BatchNorm2d(codesize, momentum=1, affine=True)
        self.norm4 = nn.BatchNorm2d(1, momentum=1, affine=True)
        
        # Decoder specification
        self.dconv1 = nn.ConvTranspose2d(codesize, 32, 3, stride=2) 
        
        self.dconv2 = nn.ConvTranspose2d(32, 64, 2, stride=2)
        self.dconv3 = nn.ConvTranspose2d(64, 1, 2, stride=2)
        self.dconv4 = nn.ConvTranspose2d(codesize, codesize, kernel_size=(3, 2), stride=2)
        self.conv8 = nn.Conv2d(codesize, codesize, 3, padding=1)
        self.conv9 = nn.Conv2d(32, 32, 3, padding=1)
        self.conv10 = nn.Conv2d(64, 64, 3, padding=1)

    def forward(self, x):
        code = self.encode(x)
        out = self.decode(code)
        return out, code
    
    def encode(self, x):
        #print("Encode...")
        #print("x", x.shape)
        x = F.relu(self.norm1(self.conv1(x)))
        #print("conv 1", x.shape)
        x = F.relu(self.norm1(self.conv4(x)))
        #print("conv 4", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm2(self.conv2(x)))
        #print("conv 2", x.shape)
        x = F.relu(self.norm2(self.conv5(x)))
        #print("conv 5", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = F.relu(self.norm3(self.conv3(x)))
        #print("conv 3", x.shape)
        x = F.relu(self.norm3(self.conv6(x)))
        #print("conv 6", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.sigmoid(self.norm3(self.conv7(x)))
        #print("conv 7", x.shape)
        x = self.pool(x)
        #print("pool", x.shape)
        x = torch.squeeze(x)
        #print("squeeze", x.shape)
        
        # Aplica la base de la RD
        x = torch.mm(x,self.krd)   
        
        return x
    
    def decode(self, x):
        
        #X‚ñ≤ =  V‚ñ≤T V‚ñ≤ Xin para reconstruir el input pero truncado
        x = torch.mm(x,torch.t(self.krd)) 
        
        #print("Decode...")
        #print("xin", x.shape)
        x = torch.unsqueeze(x, 2)
        #print("unsqueeze", x.shape)
        x = torch.unsqueeze(x, 3)
        #print("unsqueeze", x.shape)
        x = F.relu(self.norm3(self.dconv4(x)))
        #print("dconv4", x.shape)
        x = F.relu(self.norm3(self.conv8(x)))
        #print("conv8", x.shape)
        x = F.relu(self.norm2(self.dconv1(x)))
        #print("dconv1", x.shape)
        x = F.relu(self.norm2(self.conv9(x)))
        #print("conv9", x.shape)
        x = F.relu(self.norm1(self.dconv2(x)))
        #print("dconv2", x.shape)
        x = F.relu(self.norm1(self.conv10(x)))
        #print("conv10", x.shape)
        x = self.norm4(self.dconv3(x))
        #print("dconv3", x.shape)
        x = torch.tanh(x)
        #print("tanh", x.shape)
        return x

"""# **ENCODER SIMPLE CON RED FULLY CONECTED**"""

class EncoderFC(nn.Module):
    def __init__(self,Din,dout):
        super().__init__()
        self.Din = Din
        self.dout = dout
        
        self.encode = nn.Sequential(
            nn.Linear(self.Din, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(True),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(True), 
            nn.Linear(64, 12), 
            nn.BatchNorm1d(12),
            nn.ReLU(True), 
            nn.Linear(12, self.dout),
            nn.BatchNorm1d(self.dout))
        
    def forward(self, x):
        #print("x", x)
        #print("type x", type(x))
        code = self.encode(x)
        #print("code", code)
        # out = self.decoder(code)
        return code

# Decoder FC
class DecoderFC(nn.Module):
    def __init__(self,Din,dout):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.BatchNorm1d(dout),
            nn.Linear(dout, 12), 
            nn.ReLU(True),
            #nn.Dropout(0.2),
            nn.Linear(12, 64),
            nn.ReLU(True),
            #nn.Dropout(0.2),
            nn.Linear(64, 128),
            nn.ReLU(True), 
            #nn.Dropout(0.2),
            nn.Linear(128, Din), 
            nn.Tanh())

    def forward(self, x):
        out = self.decoder(x)
        return out

# **ENCODER RESIDUAL**

class BloqueResidual(nn.Module):
    def __init__(self, in_features, out_features):
        super(BloqueResidual, self).__init__()
        self.fc1 = nn.Linear(in_features, out_features)
        self.fc2 = nn.Linear(out_features, out_features)

    def forward(self, x):
        residual = x
        out = self.fc1(x)
        out = nn.functional.relu(out)
        out = self.fc2(out)
        out += residual
        out = nn.functional.relu(out)
        return out
    
class ResidualEncoder(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(ResidualEncoder, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.residual_blocks = nn.Sequential(
            BloqueResidual(hidden_size, hidden_size),
            BloqueResidual(hidden_size, hidden_size),
            BloqueResidual(hidden_size, hidden_size),
        )
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out = self.fc1(x)
        out = nn.functional.relu(out)
        out = self.residual_blocks(out)
        out = self.fc2(out)
        return out